# AI2025実験の分析結果

## 1. 配分額の分析結果（AI2025_payoffAvg.Rmd）

### 記述統計
- AI条件の平均配分額: 675.47円（SD = 238.80）
- コントロール条件の平均配分額: 684.78円（SD = 228.03）

### 統計的検定
- t検定結果:
  - t値 = -0.56066
  - 自由度 = 776.97
  - p値 = 0.5752
  - 95%信頼区間: [-41.89117, 23.27814]
  - 結果の解釈: 条件間で統計的に有意な差は見られなかった（p > .05）

## 2. 選択行動の分析結果（AI2025_Hinokentei.Rmd）

### 記述統計
- AI条件の全体の選択数: 435 
- AI条件のYの選択数: 110 (25.29%)
- コントロール条件の全体の選択数: 360 
- コントロール条件のYの選択数: 84 (23.33%)

### 統計的検定
- カイ二乗検定結果:
  - χ²値 = 0.30866
  - 自由度 = 1
  - p値 = 0.5785
  - 結果の解釈: 条件間で選択比率に統計的に有意な差は見られなかった（p > .05） 

## 3. 社会的選好パラメータの推定結果（AI2025_socialPreference_analysis.Rmd）

### 記述統計

### 推定モデル
効用関数：
u_D(πD, πR) = (1 - αs - βr)πD + (αs + βr)πR

ここで：
- πD：独裁者（意思決定者）の利得
- πR：受け手の利得
- s：不利な不平等指標 (πR > πD のとき1、それ以外0)
- r：有利な不平等指標 (πD > πR のとき1、それ以外0)
- α, β：推定するパラメータ

### 利得構造の特徴
- πD（独裁者の利得）：
  - 最小値：140円
  - 中央値：720円
  - 最大値：1060円
- πR（受け手の利得）：
  - 最小値：140円
  - 中央値：600円
  - 最大値：1060円

### 推定における問題点

1. ラウンドごとの極端な選択パターン
- ラウンド8：X=53回, Y=0回 (Y率0%)
- ラウンド5：X=13回, Y=40回 (Y率75.5%)
- ラウンド10：X=18回, Y=35回 (Y率66.0%)
- ラウンド15：X=19回, Y=34回 (Y率64.2%)

2. 不平等指標の分布の偏り
   r（有利な不平等）
s    0   1
0    0 438  # 有利な不平等のみ
1  357   0  # 不利な不平等のみ

3. パラメータ推定の失敗
- 複数の初期値（α = 0.3, β = 0.2など）を試すも収束せず
- 推定アルゴリズム（BFGS法）が最適解を見つけられない

4. 推定手法の課題
- パラメータの制約条件が明確に設定されていない
- 交差検証による推定精度の検証が不十分
- 個人レベルでのロバスト標準誤差の未使用

5. データの質の問題
- 選択時間が短い回答の存在
- 一貫性のない選択パターン
- 外れ値の処理方法が未確立

### 改善案

1. モデルの修正案
```r
# 修正案1：相対的利得差を考慮
u_D = πD + α * min(πR - πD, 0) + β * max(πD - πR, 0)

# 修正案2：非線形効用関数
u_D = (1 - α - β)log(πD) + (α + β)log(πR)
```

2. 推定方法の改善
- パラメータの制約条件：0 ≤ α, β ≤ 1
- ラウンド効果の制御
- 極端な選択パターンを示すラウンドの扱いの検討
- 交差検証による推定精度の確認
- 個人レベルでのロバスト標準誤差の使用

3. データ品質の向上
- 最小選択時間の設定
- 一貫性チェックの導入
- 外れ値の検出と処理方法の確立
- 選択の理由や確信度の記録

4. 次のステップ
- 利得構造の詳細分析
- 修正モデルでの再推定
- 先行研究との実験設計の比較
- 異なる推定手法の比較検討

### 再推定のための要件定義

1. データ準備要件
- 入力データ形式：
  - 各プレイヤーの選択データ（30ラウンド）
  - 各ラウンドの利得ペア（πD_X, πR_X, πD_Y, πR_Y）
  - 選択時間データ
  - 一貫性チェック用の指標

2. 推定手法要件
- 最尤推定法（MLE）の実装：
  ```r
  # ランダム効用モデルの確率関数
  Pr(X|πX_D, πX_R, πY_D, πY_R; α, β, σ) = 
    exp(σ uD(πX_D, πX_R; α, β)) / 
    [exp(σ uD(πX_D, πX_R; α, β)) + exp(σ uD(πY_D, πY_R; α, β))]
  ```
- パラメータ制約：
  - 0 ≤ α, β ≤ 1
  - σ > 0（スケールパラメータ）

3. 品質管理要件
- 交差検証：
  - トレーニングデータ（80%）とテストデータ（20%）に分割
  - 予測精度の評価指標の設定
- ロバスト性チェック：
  - 個人レベルでのクラスター化されたロバスト標準誤差
  - 異なる初期値での推定結果の比較
  - 外れ値の検出と感度分析

4. 出力要件
- パラメータ推定値（α, β, σ）
- 標準誤差と信頼区間
- モデル適合度指標
- 予測精度指標
- 診断プロット

5. 実装ステップ
1) データクリーニング
   - 極端な選択パターンの特定
   - 一貫性チェック
   - 外れ値の処理
2) 推定プロセス
   - MLEの実装
   - 交差検証の実施
   - 診断チェック
3) 結果の検証
   - パラメータの解釈
   - モデル適合度の評価
   - 予測精度の確認

6. 必要なRパッケージ
- `maxLik`: 最尤推定
- `sandwich`: ロバスト標準誤差
- `caret`: 交差検証
- `ggplot2`: 診断プロット
- `dplyr`: データ操作

7. 出力ファイル
- `parameter_estimates.csv`: 推定結果
- `confidence_intervals.csv`: 信頼区間
- `diagnostic_results.csv`: 診断結果
- `elasticity_results.csv`: 弾力性分析

## 4. 選択行動の詳細分析（choice_analysis.R）

### 利害対立状況での分析
- 利害対立の定義: 独裁者の利得 > 受け手の利得
- サンプル数:
  - AI条件: 238ケース
  - Control条件: 200ケース

### 利害対立状況での選択率
- AI条件: 79.8% (SE: 2.60%)
- Control条件: 79.0% (SE: 2.88%)
- t検定結果:
  - t値 = 0.214
  - 自由度 = 420.83
  - p値 = 0.831
  - 95%信頼区間: [-0.068, 0.085]
- 効果量: Cohen's d = 0.02 [95% CI: -0.17, 0.21]
- 結果の解釈: 利害対立状況でも条件間で統計的に有意な差は見られなかった（p > .05）

## 5. 利他性の詳細分析

### 時間的変化の分析
- **前半・後半の比較**
  - AI条件
    - 前半: 4.4% (SE = 1.45%)
    - 後半: 81.0% (SE = 2.58%)
  - Control条件
    - 前半: 2.4% (SE = 1.18%)
    - 後半: 81.2% (SE = 2.82%)
  - 両条件とも後半で大幅な利他性の増加が見られた

### 個人レベルの分析
- **選択の一貫性**
  - 平均利他的選択率: 44.9% (範囲: 33.3% - 53.3%)
  - 選択の一貫性（標準偏差）: 平均 0.511 (範囲: 0.488 - 0.516)
  - 個人差は比較的小さく、条件間で大きな違いは見られなかった

### 混合効果モデル分析
- **ラウンドと条件の交互作用**
  - ラウンドの主効果: z = 8.475, p < 0.001
    - 時間経過とともに利他的選択が増加
  - 条件の主効果: z = -0.938, p = 0.348
    - AI条件とControl条件で有意な差なし
  - 交互作用: z = 1.058, p = 0.290
    - 時間経過による変化のパターンは条件間で類似

### 主な知見
1. **時間的変化**
   - 両条件とも実験後半で利他的選択が劇的に増加
   - 前半では利他的選択が非常に少なく（2-4%）、後半で急増（約81%）
   - この変化パターンは両条件で非常に類似

2. **個人差**
   - 個人間の変動は比較的小さい
   - 利他的選択率の個人差は条件に関係なく一定の範囲内
   - 選択の一貫性も条件間で類似

3. **条件効果**
   - AI条件とControl条件の間に統計的に有意な差はない
   - 効果量（Cohen's d）は0.017 [95% CI: -0.123, 0.157]と非常に小さい
   - 時間経過による変化パターンも両条件で類似

### 結論
実験参加者は条件に関係なく、時間経過とともに利他的な選択を増加させる傾向が強く見られた。
この変化は特に実験の前半と後半で顕著であり、両条件で非常に類似したパターンを示した。
AI条件とControl条件の間に実質的な差は見られず、参加者の意思決定は条件よりも時間経過の影響を強く受けていたことが示唆される。

## 6. 利他性分析

### 利他的選択の定義
- 利他的選択：受け手の利得が独裁者の利得を上回る選択
- 自己犠牲度：受け手の利得 - 独裁者の利得（利他的状況のみ）

### 分析結果
1. **利他的選択率**
   - AI条件: 45.3%
   - Control条件: 44.4%
   - 差分: +0.9%ポイント（AI条件がわずかに高い）

2. **統計的検定結果**
   - t値: 0.238
   - p値: 0.812
   - 95%信頼区間: [-0.061, 0.078]
   - 解釈: 条件間で統計的に有意な差は見られない（p > .05）

3. **自己犠牲の程度**
   - AI条件の平均自己犠牲額: 168円
   - Control条件の平均自己犠牲額: 159円
   - 差分: +9円（AI条件がわずかに高い）

### 時系列分析
1. **ラウンドごとの利他的選択率**
   - AI条件とControl条件で、ラウンドを通じて安定した利他性を示す
   - 両条件とも約45%の利他性を維持

2. **前半/後半の比較**
   - AI条件
     - 前半: 45.3%
     - 後半: 45.2%
   - Control条件
     - 前半: 44.4%
     - 後半: 44.5%
   - 解釈: 両条件とも実験を通じて安定した選択パターンを示す

3. **時系列効果の特徴**
   - 学習効果は見られない
   - 条件間の差は実験全体を通じて一貫して小さい
   - 選択の一貫性が高く、参加者は安定した判断基準を持っていた可能性がある

### 結論
- AI条件とControl条件の間で利他性に実質的な差は見られない
- 両条件とも約45%の選択が利他的（受け手の利得が高い）
- 自己犠牲の程度もほぼ同等（差は9円程度）
